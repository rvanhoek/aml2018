{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Source: https://github.com/tim5go/quora-question-pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Renee\\envs\\deeplearning\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## import packages\n",
    "########################################\n",
    "import os\n",
    "import re\n",
    "import csv\n",
    "import codecs\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from imp import reload\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from string import punctuation\n",
    "\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "import sys\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## set directories and parameters\n",
    "########################################\n",
    "BASE_DIR = 'C:/Renee/aml_keras/input/'\n",
    "EMBEDDING_FILE = BASE_DIR + 'GoogleNews-vectors-negative300.bin'\n",
    "TRAIN_DATA_FILE = BASE_DIR + 'train.csv'\n",
    "TEST_DATA_FILE = BASE_DIR + 'test.csv'\n",
    "MAX_SEQUENCE_LENGTH = 30\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.1\n",
    "\n",
    "num_lstm = np.random.randint(175, 275)\n",
    "num_dense = np.random.randint(100, 150)\n",
    "rate_drop_lstm = 0.15 + np.random.rand() * 0.25\n",
    "rate_drop_dense = 0.15 + np.random.rand() * 0.25\n",
    "\n",
    "act = 'relu'\n",
    "re_weight = True # whether to re-weight classes to fit the 17.5% share in test set\n",
    "\n",
    "STAMP = 'lstm_%d_%d_%.2f_%.2f'%(num_lstm, num_dense, rate_drop_lstm, \\\n",
    "        rate_drop_dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Renee\\aml_keras\n"
     ]
    }
   ],
   "source": [
    "!echo %cd%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indexing word vectors\n",
      "Found 3000000 word vectors of word2vec\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## index word vectors\n",
    "########################################\n",
    "print('Indexing word vectors')\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMBEDDING_FILE, \\\n",
    "        binary=True)\n",
    "print('Found %s word vectors of word2vec' % len(word2vec.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Renee\\envs\\deeplearning\\lib\\site-packages\\gensim\\matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int32 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen: 0.7118\n"
     ]
    }
   ],
   "source": [
    "result = word2vec.most_similar(positive=['woman', 'king'], negative=['man'])\n",
    "print(\"{}: {:.4f}\".format(*result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing text dataset\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## process texts in datasets\n",
    "########################################\n",
    "print('Processing text dataset')\n",
    "\n",
    "# The function \"text_to_wordlist\" is from\n",
    "# https://www.kaggle.com/currie32/quora-question-pairs/the-importance-of-cleaning-text\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "    # Clean the text, with the option to remove stopwords and to stem words.\n",
    "    \n",
    "    # Convert words to lower case and split them\n",
    "    text = text.lower().split()\n",
    "\n",
    "    # Optionally, remove stop words\n",
    "    if remove_stopwords:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "    \n",
    "    text = \" \".join(text)\n",
    "\n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e - mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # Optionally, shorten words to their stems\n",
    "    if stem_words:\n",
    "        text = text.split()\n",
    "        stemmer = SnowballStemmer('english')\n",
    "        stemmed_words = [stemmer.stem(word) for word in text]\n",
    "        text = \" \".join(stemmed_words)\n",
    "    \n",
    "    # Return a list of words\n",
    "    return(text)\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return(len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']]))))\n",
    "\n",
    "def try_apply_dict(x,dict_to_apply):\n",
    "    try:\n",
    "        return dict_to_apply[x]\n",
    "    except KeyError:\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 404290 texts in train.csv\n",
      "Found 81126 texts in test.csv\n"
     ]
    }
   ],
   "source": [
    "texts_1 = [] \n",
    "texts_2 = []\n",
    "labels = []\n",
    "\n",
    "#aplly function to clean data\n",
    "with codecs.open(TRAIN_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        texts_1.append(text_to_wordlist(values[3]))\n",
    "        texts_2.append(text_to_wordlist(values[4]))\n",
    "        labels.append(int(values[5]))\n",
    "print('Found %s texts in train.csv' % len(texts_1))\n",
    "\n",
    "train_orig =  pd.read_csv(BASE_DIR + 'train.csv', header=0)\n",
    "\n",
    "test_texts_1 = []\n",
    "test_texts_2 = []\n",
    "test_ids = []\n",
    "with codecs.open(TEST_DATA_FILE, encoding='utf-8') as f:\n",
    "    reader = csv.reader(f, delimiter=',')\n",
    "    header = next(reader)\n",
    "    for values in reader:\n",
    "        test_texts_1.append(text_to_wordlist(values[1]))\n",
    "        test_texts_2.append(text_to_wordlist(values[2]))\n",
    "        test_ids.append(values[0])\n",
    "print('Found %s texts in test.csv' % len(test_texts_1))\n",
    "\n",
    "test_orig =  pd.read_csv(BASE_DIR + 'test.csv', header=0)\n",
    "\n",
    "ques = pd.concat([train_orig[['question1', 'question2']], \\\n",
    "        test_orig[['question1', 'question2']]], axis=0).reset_index(drop='index')\n",
    "\t\t\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\t\t\n",
    "train_feat = train_orig.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test_feat = test_orig.apply(q1_q2_intersect, axis=1, raw=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['what is the step by step guide to invest in share market in india ',\n",
       " 'what is the story of kohinoor koh - i - noor diamond ',\n",
       " 'how can i increase the speed of my internet connection while using a vpn ',\n",
       " 'why am i mentally very lonely how can i solve it ',\n",
       " 'which one dissolve in water quikly sugar salt methane and carbon di oxide ',\n",
       " 'astrology : i am a capricorn sun cap moon and cap rising what does that say about me ',\n",
       " 'should i buy tiago ',\n",
       " 'how can i be a good geologist ',\n",
       " 'when do you use instead of ',\n",
       " 'motorola company : can i hack my charter motorolla dcx3400 ']"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts_1[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kopieer vraag 1 en 2 van test en train naar aparte dataframes\n",
    "df1 = train_orig[['question1']].copy()\n",
    "df2 = train_orig[['question2']].copy()\n",
    "df1_test = test_orig[['question1']].copy()\n",
    "df2_test = test_orig[['question2']].copy()\n",
    "#print(df1.head(),df2.head())\n",
    "\n",
    "# Voeg alle vragen samen in één kolom in één dataframe\n",
    "df2.rename(columns = {'question2':'question1'},inplace=True)\n",
    "df2_test.rename(columns = {'question2':'question1'},inplace=True)\n",
    "#print(df2.head())\n",
    "\n",
    "train_questions = df1.append(df2)\n",
    "train_questions = train_questions.append(df1_test)\n",
    "train_questions = train_questions.append(df2_test)\n",
    "#print(train_questions.head(), train_questions.shape)\n",
    "\n",
    "# Dubbele vragen verwijderen\n",
    "#train_questions.drop_duplicates(subset = ['qid1'],inplace=True)\n",
    "train_questions.drop_duplicates(subset = ['question1'],inplace=True)\n",
    "#print(train_questions.head(), train_questions.shape)\n",
    "\n",
    "# Plaats alle originele vragen in een dictionary\n",
    "train_questions.reset_index(inplace=True,drop=True)\n",
    "questions_dict = pd.Series(train_questions.index.values,index=train_questions.question1.values).to_dict()\n",
    "\n",
    "# Samenvoegen train en testvragen in df 'comb'\n",
    "train_cp = train_orig.copy()\n",
    "test_cp = test_orig.copy()\n",
    "train_cp.drop(['qid1','qid2'],axis=1,inplace=True)\n",
    "#print(train_cp[0:15])\n",
    "\n",
    "test_cp['is_duplicate'] = -1\n",
    "test_cp.rename(columns={'test_id':'id'},inplace=True)\n",
    "#print(test_cp[0:15])\n",
    "\n",
    "comb = pd.concat([train_cp,test_cp])\n",
    "#print(comb.head(), comb.shape)\n",
    "\n",
    "# Voeg de nr's van de vragen toe uit de dictionary met unieke vragen\n",
    "comb['q1_hash'] = comb['question1'].map(questions_dict)\n",
    "comb['q2_hash'] = comb['question2'].map(questions_dict)\n",
    "#print(comb.head(), comb.shape)\n",
    "\n",
    "# Maak een dictionary met hoe vaak elke vraag voorkomt\n",
    "q1_vc = comb.q1_hash.value_counts().to_dict()\n",
    "q2_vc = comb.q2_hash.value_counts().to_dict()\n",
    "#print(q1_vc[1])\n",
    "\n",
    "# Voeg toe aan dataframe hoe vaak de betreffende vraag voorkomt\n",
    "comb['q1_freq'] = comb['q1_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "comb['q2_freq'] = comb['q2_hash'].map(lambda x: try_apply_dict(x,q1_vc) + try_apply_dict(x,q2_vc))\n",
    "#print(comb.head())\n",
    "\n",
    "# Train en testdata weer uit elkaar halen en vragen in tekst verwijderen als kolom\n",
    "train_comb = comb[comb['is_duplicate'] >= 0][['id','q1_hash','q2_hash','q1_freq','q2_freq','is_duplicate']]\n",
    "test_comb = comb[comb['is_duplicate'] < 0][['id','q1_hash','q2_hash','q1_freq','q2_freq']]\n",
    "#print(comb[0:15],train_comb[0:15])\n",
    "\n",
    "# Frequencies voorkomen vragen los opslaan\n",
    "train_q1_freq = train_comb['q1_freq']\n",
    "train_q2_freq = train_comb['q2_freq']\n",
    "test_q1_freq = test_comb['q1_freq']\n",
    "test_q2_freq = test_comb['q2_freq']\n",
    "#print(train_q2_freq[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 85518 unique tokens\n",
      "Shape of data tensor: (404290, 30)\n",
      "Shape of label tensor: (404290,)\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\n",
    "\n",
    "# The Tokenizer stores everything in the word_index during fit_on_texts\n",
    "tokenizer.fit_on_texts(texts_1 + texts_2 + test_texts_1 + test_texts_2)\n",
    "\n",
    "# When calling the texts_to_sequences method, only the top num_words are considered\n",
    "sequences_1 = tokenizer.texts_to_sequences(texts_1)\n",
    "sequences_2 = tokenizer.texts_to_sequences(texts_2)\n",
    "test_sequences_1 = tokenizer.texts_to_sequences(test_texts_1)\n",
    "test_sequences_2 = tokenizer.texts_to_sequences(test_texts_2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Found %s unique tokens' % len(word_index))\n",
    "\n",
    "# Apply padding\n",
    "data_1 = pad_sequences(sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "data_2 = pad_sequences(sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "labels = np.array(labels)\n",
    "print('Shape of data tensor:', data_1.shape)\n",
    "print('Shape of label tensor:', labels.shape)\n",
    "\n",
    "test_data_1 = pad_sequences(test_sequences_1, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_data_2 = pad_sequences(test_sequences_2, maxlen=MAX_SEQUENCE_LENGTH)\n",
    "test_ids = np.array(test_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['what is the step by step guide to invest in share market in india '] ['what is the step by step guide to invest in share market '] [[2, 3, 1, 1219, 58, 1219, 2613, 7, 570, 8, 758, 376, 8, 36]] [[2, 3, 1, 1219, 58, 1219, 2613, 7, 570, 8, 758, 376]] [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    2    3    1 1219   58 1219 2613    7  570    8  758  376\n",
      "     8   36]] [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    2    3    1 1219   58 1219 2613    7  570    8\n",
      "   758  376]] [0]\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "texts_1[0:1],\n",
    "texts_2[0:1],\n",
    "sequences_1[0:1],\n",
    "sequences_2[0:1],\n",
    "data_1[0:1],\n",
    "data_2[0:1],\n",
    "labels[0:1]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preparing embedding matrix\n",
      "Null word embeddings: 37391\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## prepare embeddings\n",
    "########################################\n",
    "\n",
    "print('Preparing embedding matrix')\n",
    "\n",
    "nb_words = min(MAX_NB_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((nb_words, EMBEDDING_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.08007812,  0.10498047,  0.04980469,  0.0534668 , -0.06738281,\n",
       "        -0.12060547,  0.03515625, -0.11865234,  0.04394531,  0.03015137,\n",
       "        -0.05688477, -0.07617188,  0.01287842,  0.04980469, -0.08496094,\n",
       "        -0.06347656,  0.00628662, -0.04321289,  0.02026367,  0.01330566,\n",
       "        -0.01953125,  0.09277344, -0.171875  , -0.00131989,  0.06542969,\n",
       "         0.05834961, -0.08251953,  0.0859375 , -0.00318909,  0.05859375,\n",
       "        -0.03491211, -0.0123291 , -0.0480957 , -0.00302124,  0.05639648,\n",
       "         0.01495361, -0.07226562, -0.05224609,  0.09667969,  0.04296875,\n",
       "        -0.03540039, -0.07324219,  0.03271484, -0.06176758,  0.00787354,\n",
       "         0.0035553 , -0.00878906,  0.0390625 ,  0.03833008,  0.04443359,\n",
       "         0.06982422,  0.01263428, -0.00445557, -0.03320312, -0.04272461,\n",
       "         0.09765625, -0.02160645, -0.0378418 ,  0.01190186, -0.01391602,\n",
       "        -0.11328125,  0.09326172, -0.03930664, -0.11621094,  0.02331543,\n",
       "        -0.01599121,  0.02636719,  0.10742188, -0.00466919,  0.09619141,\n",
       "         0.0279541 , -0.05395508,  0.08544922, -0.03686523, -0.02026367,\n",
       "        -0.08544922,  0.125     ,  0.14453125,  0.0267334 ,  0.15039062,\n",
       "         0.05273438, -0.18652344,  0.08154297, -0.01062012, -0.03735352,\n",
       "        -0.07324219, -0.07519531,  0.03613281, -0.13183594,  0.00616455,\n",
       "         0.05078125,  0.04516602,  0.0100708 , -0.15039062, -0.06005859,\n",
       "         0.05761719, -0.00692749,  0.01586914, -0.0213623 ,  0.10351562,\n",
       "        -0.00029182, -0.046875  , -0.01635742, -0.07861328, -0.06933594,\n",
       "         0.01635742, -0.03149414, -0.01373291, -0.03662109, -0.08886719,\n",
       "        -0.0480957 , -0.01318359, -0.07177734,  0.00588989, -0.04614258,\n",
       "         0.03979492,  0.10058594, -0.04931641,  0.07568359,  0.03881836,\n",
       "        -0.16699219, -0.09619141, -0.10107422,  0.02905273, -0.05786133,\n",
       "        -0.01928711, -0.04296875, -0.08398438, -0.01989746,  0.05151367,\n",
       "         0.00848389, -0.03613281, -0.14941406, -0.01855469, -0.03637695,\n",
       "        -0.07666016, -0.03955078, -0.06152344, -0.02001953,  0.04150391,\n",
       "         0.03686523, -0.07226562,  0.00592041, -0.06298828,  0.00738525,\n",
       "        -0.01586914,  0.01611328, -0.01452637,  0.00772095,  0.10107422,\n",
       "        -0.00558472,  0.01428223, -0.07617188,  0.05639648, -0.01293945,\n",
       "         0.03063965, -0.02490234, -0.09863281,  0.0324707 , -0.02807617,\n",
       "        -0.08105469,  0.02062988,  0.01611328, -0.04199219, -0.03491211,\n",
       "        -0.03759766,  0.05493164,  0.01373291,  0.02685547, -0.05859375,\n",
       "        -0.07177734, -0.12011719, -0.02282715, -0.1640625 , -0.00361633,\n",
       "        -0.05981445,  0.07080078, -0.07714844,  0.05175781, -0.04296875,\n",
       "        -0.04833984,  0.0300293 , -0.06591797, -0.03173828, -0.04882812,\n",
       "        -0.03491211,  0.05883789, -0.01464844,  0.18066406,  0.05688477,\n",
       "         0.05249023,  0.05786133,  0.11669922,  0.05200195, -0.0534668 ,\n",
       "         0.01867676, -0.015625  ,  0.00576782, -0.07324219, -0.11621094,\n",
       "         0.04052734,  0.0625    , -0.04321289,  0.01055908,  0.02172852,\n",
       "         0.04248047,  0.03271484,  0.04418945,  0.05761719,  0.02612305,\n",
       "        -0.01831055, -0.02697754, -0.00674438,  0.00509644, -0.11621094,\n",
       "         0.00364685,  0.05761719, -0.05957031, -0.08837891,  0.0135498 ,\n",
       "         0.04541016, -0.04638672, -0.0177002 , -0.0625    ,  0.03442383,\n",
       "        -0.02416992,  0.03088379,  0.09570312,  0.07958984,  0.03930664,\n",
       "         0.0279541 , -0.0859375 ,  0.08105469,  0.06640625, -0.00041962,\n",
       "        -0.06933594,  0.03588867, -0.03417969,  0.04492188, -0.00772095,\n",
       "        -0.00741577, -0.04760742,  0.01397705, -0.09960938,  0.0246582 ,\n",
       "        -0.09960938,  0.11474609,  0.03173828,  0.02209473,  0.07226562,\n",
       "         0.03686523,  0.02563477,  0.01367188, -0.02734375,  0.00592041,\n",
       "        -0.06738281,  0.05053711, -0.02832031, -0.04516602, -0.01733398,\n",
       "         0.02111816,  0.03515625, -0.04296875,  0.06640625,  0.12207031,\n",
       "         0.12353516,  0.0039978 ,  0.04516602, -0.01855469,  0.04833984,\n",
       "         0.04516602,  0.08691406,  0.02941895,  0.03759766,  0.03442383,\n",
       "        -0.07373047, -0.0402832 , -0.14648438, -0.02441406, -0.01953125,\n",
       "         0.0065918 , -0.0018158 , -0.01092529,  0.09326172,  0.06542969,\n",
       "         0.01843262, -0.09326172, -0.01574707, -0.07128906, -0.08935547,\n",
       "        -0.07128906, -0.03015137, -0.01300049,  0.01635742, -0.01831055,\n",
       "         0.01483154,  0.00500488,  0.00366211,  0.04760742, -0.06884766]])"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# embedding matrix zijn de 300 dimensies vectoren van alle voorkomende woorden\n",
    "embedding_matrix.shape\n",
    "# voorbeeld\n",
    "embedding_matrix[1:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[    0     0     0     0     0     0     0     0     0     0     0     0\n",
      "      0     0     0     0     0     0     0    13     4  2712 11042  1267\n",
      "     14  1581  1267    25    63     5]] [[   0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "     0    0    0    0    0    0    0    0    0    0    2   11 6105   14\n",
      "  1581 1267]]\n",
      "[0 0 1 0 0 1 0 0 0 0]\n",
      "[ 0  0  0  0  0  4  0  0  0  0  0  0  7  2  0  0  0  6  0  0 10  0  0  0\n",
      "  0  0 26  0  0  0  0  2  0  0  0  0  0  0  0  0 11  0  0  0  0  3  0  0\n",
      "  0  0  0  0  0  0 10  0  0  0  0  2  0  2  0  0  0  0  0  0  0  5  5  0\n",
      "  0  1  0  0  3  8  0  0  0  0  0 22  0  1  1  0  0  0  0  0  0  0  0  0\n",
      "  0  1  0 10]\n",
      "[ 5  3 35  1  2  7  1  1  2  5]\n"
     ]
    }
   ],
   "source": [
    "########################################\n",
    "## sample train/validation data\n",
    "########################################\n",
    "#np.random.seed(1234)\n",
    "\n",
    "# Random split nemen van train data naar een train en validatie set\n",
    "perm = np.random.permutation(len(data_1))\n",
    "idx_train = perm[:int(len(data_1)*(1-VALIDATION_SPLIT))]\n",
    "idx_val = perm[int(len(data_1)*(1-VALIDATION_SPLIT)):]\n",
    "\n",
    "data_1_train = np.vstack((data_1[idx_train], data_2[idx_train]))\n",
    "data_2_train = np.vstack((data_2[idx_train], data_1[idx_train]))\n",
    "labels_train = np.concatenate((labels[idx_train], labels[idx_train]))\n",
    "#intersect van de vragen\n",
    "feat_train   = np.concatenate((train_feat[idx_train], train_feat[idx_train]))\n",
    "#frequencies\n",
    "q1_train     = np.concatenate((train_q1_freq[idx_train], train_q2_freq[idx_train]))\n",
    "q2_train     = np.concatenate((train_q2_freq[idx_train], train_q1_freq[idx_train])) \n",
    "\n",
    "#print(data_1_train[0:1],data_2_train[0:1])\n",
    "#print(labels_train[0:10])\n",
    "#print(feat_train[0:10])\n",
    "#print(q1_train[0:10])\n",
    "\n",
    "# Zelfde voor validatieset\n",
    "data_1_val = np.vstack((data_1[idx_val], data_2[idx_val]))\n",
    "data_2_val = np.vstack((data_2[idx_val], data_1[idx_val]))\n",
    "labels_val = np.concatenate((labels[idx_val], labels[idx_val]))\n",
    "feat_val   = np.concatenate((train_feat[idx_val], train_feat[idx_val]))\n",
    "q1_val     = np.concatenate((train_q1_freq[idx_val], train_q2_freq[idx_val]))\n",
    "q2_val     = np.concatenate((train_q2_freq[idx_val], train_q1_freq[idx_val]))\n",
    "\n",
    "#???\n",
    "weight_val = np.ones(len(labels_val))\n",
    "if re_weight:\n",
    "    weight_val *= 0.472001959\n",
    "    weight_val[labels_val==0] = 1.309028344\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## define the model structure\n",
    "########################################\n",
    "embedding_layer = Embedding(nb_words,\n",
    "        EMBEDDING_DIM,\n",
    "        weights=[embedding_matrix],\n",
    "        input_length=MAX_SEQUENCE_LENGTH,\n",
    "        trainable=False)\n",
    "lstm_layer = LSTM(num_lstm, dropout=rate_drop_lstm, recurrent_dropout=rate_drop_lstm)\n",
    "\n",
    "sequence_1_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_1 = embedding_layer(sequence_1_input)\n",
    "x1 = lstm_layer(embedded_sequences_1)\n",
    "\n",
    "sequence_2_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\n",
    "embedded_sequences_2 = embedding_layer(sequence_2_input)\n",
    "y1 = lstm_layer(embedded_sequences_2)\n",
    "\n",
    "z1 = Input(shape=(1,), dtype='float32')\n",
    "\n",
    "a1 = Input(shape=(1,), dtype='float32')\n",
    "b1 = Input(shape=(1,), dtype='float32')\n",
    "\n",
    "merged = concatenate([x1, y1, z1, a1, b1])\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "merged = Dense(num_dense, activation=act)(merged)\n",
    "merged = Dropout(rate_drop_dense)(merged)\n",
    "merged = BatchNormalization()(merged)\n",
    "\n",
    "preds = Dense(1, activation='sigmoid')(merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## add class weight\n",
    "########################################\n",
    "if re_weight:\n",
    "    class_weight = {0: 1.309028344, 1: 0.472001959}\n",
    "else:\n",
    "    class_weight = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## train the model\n",
    "########################################\n",
    "model = Model(inputs=[sequence_1_input, sequence_2_input, z1, a1, b1], \\\n",
    "        outputs=preds)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "        optimizer='nadam',\n",
    "        metrics=['acc'])\n",
    "#model.summary()\n",
    "print(STAMP)\n",
    "\n",
    "early_stopping =EarlyStopping(monitor='val_loss', patience=3)\n",
    "bst_model_path = STAMP + '.h5'\n",
    "model_checkpoint = ModelCheckpoint(bst_model_path, save_best_only=True, save_weights_only=True)\n",
    "\n",
    "hist = model.fit([data_1_train, data_2_train, feat_train, q1_train, q2_train], labels_train, \\\n",
    "        validation_data=([data_1_val, data_2_val, feat_val, q1_val, q2_val], labels_val, weight_val), \\\n",
    "        epochs=200, batch_size=2048, shuffle=True, \\\n",
    "        class_weight=class_weight, callbacks=[early_stopping, model_checkpoint])\n",
    "\n",
    "model.load_weights(bst_model_path)\n",
    "bst_val_score = min(hist.history['val_loss'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "## make the submission\n",
    "########################################\n",
    "print('Start making the submission before fine-tuning')\n",
    "\n",
    "preds = model.predict([test_data_1, test_data_2, test_feat, test_q1_freq, test_q2_freq], batch_size=8192, verbose=1)\n",
    "preds += model.predict([test_data_2, test_data_1, test_feat, test_q1_freq, test_q2_freq], batch_size=8192, verbose=1)\n",
    "preds /= 2\n",
    "\n",
    "submission = pd.DataFrame({'test_id':test_ids, 'is_duplicate':preds.ravel()})\n",
    "submission.to_csv('%.4f_'%(bst_val_score)+STAMP+'.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
