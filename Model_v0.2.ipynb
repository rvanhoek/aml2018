{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Renee\\envs\\deeplearning\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "import codecs\n",
    "import csv\n",
    "from gensim.models import KeyedVectors\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Renée\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Renée\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Renée\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Eenmalig downloaden\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "\n",
    "# Voorkomen onterechte warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie voor lemmatizing\n",
    "\"\"\"\n",
    "\"Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language.\"\n",
    "Lemmatization is waarschijnlijk beter dan stemming in ons project omdat de betekenis van de woorden erg belangrijk is \n",
    "\"\"\"\n",
    "\n",
    "def cleanSentence(text):\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e ?-? ?mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # tokenize\n",
    "    words=text.split()\n",
    "    \n",
    "    # lemmatize\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lem_sentence=[]\n",
    "    for word in words:   \n",
    "        lem_sentence.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        lem_sentence.append(\" \")\n",
    "    \n",
    "    return \"\".join(lem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toepassen van lemmatization functie op Train data\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "train_data['q1_clean'] = train_data['question1'].apply(cleanSentence)\n",
    "train_data['q2_clean'] = train_data['question2'].apply(cleanSentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "test_data['q1_clean'] = test_data['question1'].apply(cleanSentence)\n",
    "test_data['q2_clean'] = test_data['question2'].apply(cleanSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77325 unique words\n"
     ]
    }
   ],
   "source": [
    "# The Tokenizer stores everything in the word_index during fit_on_texts\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "\n",
    "# Omzitten naar list om alle woorden in de fit_on_texts functie te krijgen\n",
    "train_data_q1 = train_data['q1_clean'].tolist()\n",
    "train_data_q2 = train_data['q2_clean'].tolist()\n",
    "test_data_q1 = test_data['q1_clean'].tolist()\n",
    "test_data_q2 = test_data['q2_clean'].tolist()\n",
    "\n",
    "tokenizer.fit_on_texts(train_data_q1+\n",
    "                       train_data_q2+\n",
    "                       test_data_q1+\n",
    "                       test_data_q2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('%s unique words' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "# Locatie embedding file (misschien nog andere gebruiken dan deze)\n",
    "EMB_FILE = 'C:/Renee/aml2018/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMB_FILE, \\\n",
    "        binary=True)\n",
    "print('%s word vectors' % len(word2vec.vocab))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 250000\n",
    "EMB_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without embedding: 37254\n"
     ]
    }
   ],
   "source": [
    "# Embedding matrix maken (300 dimension vectoren voor alle woorden uit de index)\n",
    "n_words = min(MAX_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((n_words, EMB_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        \n",
    "print('Words without embedding: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77326, 300)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_SEQUENCE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of data tensor: (404290, 25)\n"
     ]
    }
   ],
   "source": [
    "train_seq_1 = tokenizer.texts_to_sequences(train_data['q1_clean'])\n",
    "train_seq_2 = tokenizer.texts_to_sequences(train_data['q2_clean'])\n",
    "test_seq_1 = tokenizer.texts_to_sequences(test_data['q1_clean'])\n",
    "test_seq_2 = tokenizer.texts_to_sequences(test_data['q2_clean'])\n",
    "\n",
    "# Padding in sequences when sentences smaller than 25 words\n",
    "train_1 = pad_sequences(train_seq_1, maxlen=LENGTH_SEQUENCE)\n",
    "train_2 = pad_sequences(train_seq_2, maxlen=LENGTH_SEQUENCE)\n",
    "\n",
    "print('Shape of tensor:', train_1.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: (81126, 25)\n"
     ]
    }
   ],
   "source": [
    "test_1 = pad_sequences(test_seq_1, maxlen=LENGTH_SEQUENCE)\n",
    "test_2 = pad_sequences(test_seq_2, maxlen=LENGTH_SEQUENCE)\n",
    "\n",
    "print('Shape of tensor:', test_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Renee\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:2: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Labels naar numpy array\n",
    "labels = train_data['is_duplicate']\n",
    "labels = labels.as_matrix()\n",
    "\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a validation split of 10%\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
