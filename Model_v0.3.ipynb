{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Renee\\envs\\deeplearning\\lib\\site-packages\\gensim\\utils.py:1212: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import LancasterStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import gensim\n",
    "from nltk.corpus import brown\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import re\n",
    "\n",
    "import codecs\n",
    "import csv\n",
    "from gensim.models import KeyedVectors\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Keras\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.models import Model\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Renée\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Renée\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     C:\\Users\\Renée\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Eenmalig downloaden\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('brown')\n",
    "\n",
    "# Voorkomen onterechte warnings\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functie voor lemmatizing\n",
    "\"\"\"\n",
    "\"Lemmatization, unlike Stemming, reduces the inflected words properly ensuring that the root word belongs to the language.\"\n",
    "Lemmatization is waarschijnlijk beter dan stemming in ons project omdat de betekenis van de woorden erg belangrijk is \n",
    "\"\"\"\n",
    "\n",
    "def cleanSentence(text):\n",
    "    \n",
    "    text = str(text).lower()\n",
    "    \n",
    "    # Clean the text\n",
    "    text = re.sub(r\"[^A-Za-z0-9^,!.\\/'+-=]\", \" \", text)\n",
    "    text = re.sub(r\"what's\", \"what is \", text)\n",
    "    text = re.sub(r\"\\'s\", \" \", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have \", text)\n",
    "    text = re.sub(r\"can't\", \"cannot \", text)\n",
    "    text = re.sub(r\"n't\", \" not \", text)\n",
    "    text = re.sub(r\"i'm\", \"i am \", text)\n",
    "    text = re.sub(r\"\\'re\", \" are \", text)\n",
    "    text = re.sub(r\"\\'d\", \" would \", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will \", text)\n",
    "    text = re.sub(r\",\", \" \", text)\n",
    "    text = re.sub(r\"\\.\", \" \", text)\n",
    "    text = re.sub(r\"!\", \" ! \", text)\n",
    "    text = re.sub(r\"\\/\", \" \", text)\n",
    "    text = re.sub(r\"\\^\", \" ^ \", text)\n",
    "    text = re.sub(r\"\\+\", \" + \", text)\n",
    "    text = re.sub(r\"\\-\", \" - \", text)\n",
    "    text = re.sub(r\"\\=\", \" = \", text)\n",
    "    text = re.sub(r\"'\", \" \", text)\n",
    "    text = re.sub(r\"(\\d+)(k)\", r\"\\g<1>000\", text)\n",
    "    text = re.sub(r\":\", \" : \", text)\n",
    "    text = re.sub(r\" e g \", \" eg \", text)\n",
    "    text = re.sub(r\" b g \", \" bg \", text)\n",
    "    text = re.sub(r\" u s \", \" american \", text)\n",
    "    text = re.sub(r\"\\0s\", \"0\", text)\n",
    "    text = re.sub(r\" 9 11 \", \"911\", text)\n",
    "    text = re.sub(r\"e ?-? ?mail\", \"email\", text)\n",
    "    text = re.sub(r\"j k\", \"jk\", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    \n",
    "    # tokenize\n",
    "    words=text.split()\n",
    "    \n",
    "    # lemmatize\n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lem_sentence=[]\n",
    "    for word in words:   \n",
    "        lem_sentence.append(wordnet_lemmatizer.lemmatize(word, pos=\"v\"))\n",
    "        lem_sentence.append(\" \")\n",
    "    \n",
    "    return \"\".join(lem_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Toepassen van lemmatization functie op Train data\n",
    "\n",
    "train_data = pd.read_csv('train_data.csv')\n",
    "\n",
    "train_data['q1_clean'] = train_data['question1'].apply(cleanSentence)\n",
    "train_data['q2_clean'] = train_data['question2'].apply(cleanSentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv('test_data.csv')\n",
    "\n",
    "test_data['q1_clean'] = test_data['question1'].apply(cleanSentence)\n",
    "test_data['q2_clean'] = test_data['question2'].apply(cleanSentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Word index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "77325 unique words\n"
     ]
    }
   ],
   "source": [
    "# The Tokenizer stores everything in the word_index during fit_on_texts\n",
    "tokenizer = Tokenizer(num_words=200000)\n",
    "\n",
    "# Omzitten naar list om alle woorden in de fit_on_texts functie te krijgen\n",
    "train_data_q1 = train_data['q1_clean'].tolist()\n",
    "train_data_q2 = train_data['q2_clean'].tolist()\n",
    "test_data_q1 = test_data['q1_clean'].tolist()\n",
    "test_data_q2 = test_data['q2_clean'].tolist()\n",
    "\n",
    "tokenizer.fit_on_texts(train_data_q1+\n",
    "                       train_data_q2+\n",
    "                       test_data_q1+\n",
    "                       test_data_q2)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('%s unique words' % len(word_index))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Embedding matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3000000 word vectors\n"
     ]
    }
   ],
   "source": [
    "# Locatie embedding file (misschien nog andere gebruiken dan deze)\n",
    "EMB_FILE = 'C:/Renee/aml2018/GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "word2vec = KeyedVectors.load_word2vec_format(EMB_FILE, binary=True)\n",
    "print('%s word vectors' % len(word2vec.vocab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_WORDS = 250000\n",
    "EMB_DIM = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words without embedding: 37254\n"
     ]
    }
   ],
   "source": [
    "# Embedding matrix maken (300 dimension vectoren voor alle woorden uit de index)\n",
    "n_words = min(MAX_WORDS, len(word_index))+1\n",
    "\n",
    "embedding_matrix = np.zeros((n_words, EMB_DIM))\n",
    "for word, i in word_index.items():\n",
    "    if word in word2vec.vocab:\n",
    "        embedding_matrix[i] = word2vec.word_vec(word)\n",
    "        \n",
    "print('Words without embedding: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(77326, 300)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH_SEQUENCE = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: (404290, 25)\n"
     ]
    }
   ],
   "source": [
    "# Train data to sequences\n",
    "\n",
    "train_seq_1 = tokenizer.texts_to_sequences(train_data['q1_clean'])\n",
    "train_seq_2 = tokenizer.texts_to_sequences(train_data['q2_clean'])\n",
    "\n",
    "# Padding in sequences when sentences smaller than 25 words\n",
    "train_data_1 = pad_sequences(train_seq_1, maxlen=LENGTH_SEQUENCE)\n",
    "train_data_2 = pad_sequences(train_seq_2, maxlen=LENGTH_SEQUENCE)\n",
    "\n",
    "print('Shape of tensor:', train_data_1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor: (81126, 25)\n"
     ]
    }
   ],
   "source": [
    "# Test data to sequences\n",
    "\n",
    "test_seq_1 = tokenizer.texts_to_sequences(test_data['q1_clean'])\n",
    "test_seq_2 = tokenizer.texts_to_sequences(test_data['q2_clean'])\n",
    "\n",
    "# Padding in sequences when sentences smaller than 25 words\n",
    "test_data_1 = pad_sequences(test_seq_1, maxlen=LENGTH_SEQUENCE)\n",
    "test_data_2 = pad_sequences(test_seq_2, maxlen=LENGTH_SEQUENCE)\n",
    "\n",
    "print('Shape of tensor:', test_data_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of label tensor: (404290,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Renee\\envs\\deeplearning\\lib\\site-packages\\ipykernel_launcher.py:3: FutureWarning: Method .as_matrix will be removed in a future version. Use .values instead.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "# Labels naar numpy array\n",
    "labels = train_data['is_duplicate']\n",
    "labels = labels.as_matrix()\n",
    "\n",
    "print('Shape of label tensor:', labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Validation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "SPLIT = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a validation split of 10%\n",
    "\n",
    "# Index van train en validation set\n",
    "p = np.random.permutation(len(train_data_1))\n",
    "train_index = p[:int(len(train_data_1)*(1-SPLIT))]\n",
    "val_index = p[int(len(train_data_1)*(1-SPLIT)):]\n",
    "\n",
    "# train set\n",
    "train_1 = np.vstack((train_data_1[train_index], train_data_2[train_index]))\n",
    "train_2 = np.vstack((train_data_2[train_index], train_data_1[train_index]))\n",
    "\n",
    "# validation set\n",
    "val_1 = np.vstack((train_data_1[val_index], train_data_2[val_index]))\n",
    "val_2 = np.vstack((train_data_2[val_index], train_data_1[val_index]))\n",
    "\n",
    "# Inclusief labels\n",
    "train_lab = np.concatenate((labels[train_index], labels[train_index]))\n",
    "val_lab = np.concatenate((labels[val_index], labels[val_index]))\n",
    "\n",
    "# Weights\n",
    "val_weight = np.ones(len(val_lab))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train tensor: (727722, 25)\n",
      "Shape of validation tensor: (80858, 25)\n"
     ]
    }
   ],
   "source": [
    "print('Shape of train tensor:', train_1.shape)\n",
    "print('Shape of validation tensor:', val_1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Submission"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (3.6)",
   "language": "python",
   "name": "deeplearning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
